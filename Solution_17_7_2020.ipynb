{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1 - Demonstrate POS Tagging on any news article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk \n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize, sent_tokenize \n",
    "stop_words = set(stopwords.words('english')) \n",
    "  \n",
    "// Dummy text \n",
    "txt = \"Sukanya, Rajib and Naba are my good friends. \" \\ \n",
    "    \"Sukanya is getting married next year. \" \\ \n",
    "    \"Marriage is a big step in oneâ€™s life.\" \\ \n",
    "    \"It is both exciting and frightening. \" \\ \n",
    "    \"But friendship is a sacred bond between people.\" \\ \n",
    "    \"It is a special kind of love between us. \" \\ \n",
    "    \"Many of you must have tried searching for a friend \"\\ \n",
    "    \"but never found the right one.\"\n",
    "  \n",
    "tokenized = sent_tokenize(txt) \n",
    "for i in tokenized: \n",
    "    wordsList = nltk.word_tokenize(i) \n",
    "    wordsList = [w for w in wordsList if not w in stop_words]   \n",
    "    tagged = nltk.pos_tag(wordsList) \n",
    "    print(tagged) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2 - Topic Classification on News Article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('news', 'r') as f:\n",
    "    text = f.read()\n",
    "    news = text.split(\"\\n\\n\")\n",
    "    count = {'sport': 0, 'world': 0, \"us\": 0, \"business\": 0, \"health\": 0, \"entertainment\": 0, \"sci_tech\": 0}\n",
    "    for news_item in news:\n",
    "        lines = news_item.split(\"\\n\")\n",
    "        print(lines[6])\n",
    "        file_to_write = open('data/' + lines[6] + '/' + str(count[lines[6]]) + '.txt', 'w+')\n",
    "        count[lines[6]] = count[lines[6]] + 1\n",
    "        file_to_write.write(news_item)  # python will convert \\n to os.linesep\n",
    "        file_to_write.close()with open('news', 'r') as f:\n",
    "    text = f.read()\n",
    "    news = text.split(\"\\n\\n\")\n",
    "    count = {'sport': 0, 'world': 0, \"us\": 0, \"business\": 0, \"health\": 0, \"entertainment\": 0, \"sci_tech\": 0}\n",
    "    for news_item in news:\n",
    "        lines = news_item.split(\"\\n\")\n",
    "        print(lines[6])\n",
    "        file_to_write = open('data/' + lines[6] + '/' + str(count[lines[6]]) + '.txt', 'w+')\n",
    "        count[lines[6]] = count[lines[6]] + 1\n",
    "        file_to_write.write(news_item)  # python will convert \\n to os.linesep\n",
    "        file_to_write.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "import glob\n",
    "\n",
    "category_list = [\"sport\", \"world\", \"us\", \"business\", \"health\", \"entertainment\", \"sci_tech\"]\n",
    "directory_list = [\"data/sport/*.txt\", \"data/world/*.txt\",\"data/us/*.txt\",\"data/business/*.txt\",\"data/health/*.txt\",\"data/entertainment/*.txt\",\"data/sci_tech/*.txt\",]\n",
    "\n",
    "text_files = list(map(lambda x: glob.glob(x), directory_list))\n",
    "text_files = [item for sublist in text_files for item in sublist]\n",
    "\n",
    "training_data = []\n",
    "\n",
    "\n",
    "for t in text_files:\n",
    "    f = open(t, 'r')\n",
    "    f = f.read()\n",
    "    t = f.split('\\n')\n",
    "    training_data.append({'data' : t[0] + ' ' + t[1], 'flag' : category_list.index(t[6])})\n",
    "    \n",
    "training_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "training_data = pandas.DataFrame(training_data, columns=['data', 'flag'])\n",
    "training_data.to_csv(\"train_data.csv\", sep=',', encoding='utf-8')\n",
    "print(training_data.data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "\n",
    "#GET VECTOR COUNT\n",
    "count_vect = CountVectorizer()\n",
    "X_train_counts = count_vect.fit_transform(training_data.data)\n",
    "\n",
    "#SAVE WORD VECTOR\n",
    "pickle.dump(count_vect.vocabulary_, open(\"count_vector.pkl\",\"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "#TRANSFORM WORD VECTOR TO TF IDF\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "\n",
    "#SAVE TF-IDF\n",
    "pickle.dump(tfidf_transformer, open(\"tfidf.pkl\",\"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multinomial Naive Bayes\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#clf = MultinomialNB().fit(X_train_tfidf, training_data.flag)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_train_tfidf, training_data.flag, test_size=0.25, random_state=42)\n",
    "clf = MultinomialNB().fit(X_train, y_train)\n",
    "\n",
    "#SAVE MODEL\n",
    "pickle.dump(clf, open(\"nb_model.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "category_list = [\"sport\", \"world\", \"us\", \"business\", \"health\", \"entertainment\", \"sci_tech\"]\n",
    "\n",
    "docs_new = \"Messi joins other football team\"\n",
    "docs_new = [docs_new]\n",
    "\n",
    "#LOAD MODEL\n",
    "loaded_vec = CountVectorizer(vocabulary=pickle.load(open(\"count_vector.pkl\", \"rb\")))\n",
    "loaded_tfidf = pickle.load(open(\"tfidf.pkl\",\"rb\"))\n",
    "loaded_model = pickle.load(open(\"nb_model.pkl\",\"rb\"))\n",
    "\n",
    "X_new_counts = loaded_vec.transform(docs_new)\n",
    "X_new_tfidf = loaded_tfidf.transform(X_new_counts)\n",
    "predicted = loaded_model.predict(X_new_tfidf)\n",
    "\n",
    "print(category_list[predicted[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = loaded_model.predict(X_test)\n",
    "result_bayes = pandas.DataFrame( {'true_labels': y_test,'predicted_labels': predicted})\n",
    "result_bayes.to_csv('res_bayes.csv', sep = ',')\n",
    "\n",
    "for predicted_item, result in zip(predicted, y_test):\n",
    "    print(category_list[predicted_item], ' - ', category_list[result])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix  \n",
    "\n",
    "confusion_mat = confusion_matrix(y_test,predicted)\n",
    "print(confusion_mat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "clf_neural = MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(15,), random_state=1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_train_tfidf, training_data.flag, test_size=0.25, random_state=42)\n",
    "\n",
    "clf_neural.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(clf_neural, open(\"softmax.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = clf_neural.predict(X_test)\n",
    "result_softmax = pandas.DataFrame( {'true_labels': y_test,'predicted_labels': predicted})\n",
    "result_softmax.to_csv('res_softmax.csv', sep = ',')\n",
    "\n",
    "for predicted_item, result in zip(predicted, y_test):\n",
    "    print(category_list[predicted_item], ' - ', category_list[result])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "clf_svm = svm.LinearSVC()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_train_tfidf, training_data.flag, test_size=0.25, random_state=42)\n",
    "clf_svm.fit(X_train_tfidf, training_data.flag)\n",
    "pickle.dump(clf_svm, open(\"svm.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "predicted = clf_svm.predict(X_test)\n",
    "result_svm = pandas.DataFrame( {'true_labels': y_test,'predicted_labels': predicted})\n",
    "result_svm.to_csv('res_svm.csv', sep = ',')\n",
    "for predicted_item, result in zip(predicted, y_test):\n",
    "    print(category_list[predicted_item], ' - ', category_list[result])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
